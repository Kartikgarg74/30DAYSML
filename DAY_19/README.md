# Day 19: Variational Autoencoders (VAEs)

This blog explores **Variational Autoencoders (VAEs)**, a powerful generative model that provides a probabilistic framework for generating data. Key topics include:

- **Auto-encoders**: Basic concepts of encoding and decoding data.
- **The Need for VAEs**: How VAEs extend auto-encoders for generative tasks.
- **How VAEs Work**: The encoder, decoder, and latent space sampling process.
- **Loss Function**: Combining reconstruction loss and KL divergence.
- **Advantages of VAEs**: Flexibility in generative tasks and a regularized latent space.

## Implementation
- A simple VAE is implemented in **Keras** to generate images from the MNIST dataset.
- The blog includes code for the encoder, decoder, and training of the VAE.

## Resources
- [TensorFlow VAE Tutorial](https://www.tensorflow.org/tutorials/generative/cvae)
- [Research Paper on VAEs](https://arxiv.org/abs/1312.6114)
- [Medium blog for the code](https://medium.com/@gargkartik74)

## Conclusion
VAEs offer a probabilistic and flexible approach to data generation, making them a valuable tool in deep learning.

Feel free to check out the full blog for detailed explanations and code!

